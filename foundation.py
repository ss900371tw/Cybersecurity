import streamlit as st
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import os
import io
from transformers import cached_path
# å˜—è©¦åŒ¯å…¥ pypdfï¼Œå¦‚æœæ²’æœ‰å®‰è£å‰‡æç¤º
try:
    import pypdf
except ImportError:
    pypdf = None

# --- é é¢è¨­å®š ---
st.set_page_config(page_title="Cybersecurity AI Assistant", page_icon="ğŸ›¡ï¸", layout="wide")

st.title("ğŸ›¡ï¸ Foundation-Sec-8B-Instruct Dashboard")
st.markdown("åŸºæ–¼ `fdtn-ai/Foundation-Sec-8B-Instruct` æ¨¡å‹çš„è³‡å®‰å°ˆå®¶èŠå¤©æ©Ÿå™¨äºº")

# --- å´é‚Šæ¬„è¨­å®š (åƒæ•¸èˆ‡ Token) ---
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")
    
    default_token = os.getenv("HF_TOKEN", "") 
    hf_token = st.text_input("Hugging Face Token", value=default_token, type="password", help="è«‹è¼¸å…¥æ‚¨çš„ HF Token ä»¥å­˜å–æ¨¡å‹")
    
    st.divider()
    
    # === æ–°å¢ï¼šæª”æ¡ˆä¸Šå‚³åŠŸèƒ½ ===
    st.subheader("ğŸ“‚ ä¸Šå‚³åˆ†ææª”æ¡ˆ")
    uploaded_file = st.file_uploader("ä¸Šå‚³ Logs", type=['txt', 'py', 'log', 'csv', 'md', 'json', 'pdf'])
    
    if uploaded_file and uploaded_file.type == "application/pdf" and pypdf is None:
        st.warning("å¦‚æœè¦æ”¯æ´ PDFï¼Œè«‹å®‰è£ pypdf: `pip install pypdf`")

    st.divider()

    st.subheader("æ¨¡å‹åƒæ•¸")
    system_prompt = st.text_area("System Prompt", value="You are a cybersecurity expert. If the user provides a file content, analyze it carefully.", height=100)
    max_new_tokens = st.slider("Max New Tokens", min_value=128, max_value=4096, value=1024, step=128) # å¢åŠ ä¸Šé™ä»¥å®¹ç´é•·æª”æ¡ˆåˆ†æ
    temperature = st.slider("Temperature", min_value=0.0, max_value=1.5, value=0.1, step=0.1, help="æ•¸å€¼è¶Šä½ï¼Œå›ç­”è¶Šä¿å®ˆå›ºå®šï¼›æ•¸å€¼è¶Šé«˜ï¼Œå›ç­”è¶Šæœ‰å‰µæ„ã€‚")
    repetition_penalty = st.slider("Repetition Penalty", min_value=1.0, max_value=2.0, value=1.2, step=0.1)
    
    if st.button("æ¸…é™¤å°è©±æ­·å²"):
        st.session_state.messages = []
        st.rerun()

# --- ç¡¬é«”åµæ¸¬ ---
def get_device():
    if torch.cuda.is_available():
        return "cuda"
    elif torch.backends.mps.is_available():
        return "mps"
    else:
        return "cpu"

DEVICE = get_device()
st.sidebar.markdown(f"**ç›®å‰é‹ç®—è£ç½®:** `{DEVICE}`")

# --- æ¨¡å‹è¼‰å…¥ (ä½¿ç”¨ cache é¿å…é‡è¤‡è¼‰å…¥) ---
@st.cache_resource
def load_model(model_id, token):
    if not token:
        st.error("è«‹åœ¨å´é‚Šæ¬„è¼¸å…¥ Hugging Face Token")
        return None, None
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_id, token=token)
        model = AutoModelForCausalLM.from_pretrained(
            pretrained_model_name_or_path=model_id,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            token=token,
        )
        return tokenizer, model
    except Exception as e:
        st.error(f"æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        return None, None

# åªæœ‰åœ¨æœ‰ Token æ™‚æ‰è¼‰å…¥æ¨¡å‹
if hf_token:
    MODEL_ID = "fdtn-ai/Foundation-Sec-8B-Instruct"
    with st.spinner(f"æ­£åœ¨è¼‰å…¥æ¨¡å‹ {MODEL_ID} ... (é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜)"):
        tokenizer, model = load_model(MODEL_ID, hf_token)
else:
    st.warning("è«‹å…ˆè¼¸å…¥ Hugging Face Token æ‰èƒ½é–‹å§‹ã€‚")
    st.stop()

# --- åˆå§‹åŒ– Session State (å°è©±æ­·å²) ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- æª”æ¡ˆè™•ç†å‡½æ•¸ ---
def process_file_content(uploaded_file):
    """è®€å–ä¸Šå‚³æª”æ¡ˆä¸¦è½‰ç‚ºæ–‡å­—å­—ä¸²"""
    if uploaded_file is None:
        return None
    
    file_content = ""
    try:
        # è™•ç† PDF
        if uploaded_file.type == "application/pdf":
            if pypdf:
                pdf_reader = pypdf.PdfReader(uploaded_file)
                for page in pdf_reader.pages:
                    file_content += page.extract_text() + "\n"
            else:
                return "[Error] PDF library not installed."
        # è™•ç†ç´”æ–‡å­—/ç¨‹å¼ç¢¼/Logs
        else:
            stringio = io.StringIO(uploaded_file.getvalue().decode("utf-8"))
            file_content = stringio.read()
            
        return file_content
    except Exception as e:
        return f"[Error reading file: {str(e)}]"

# --- é¡¯ç¤ºå°è©±æ­·å² ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- æ¨è«–é‚è¼¯ ---
def generate_response(prompt, history, sys_prompt, file_context=None):
    # å»ºæ§‹ç¬¦åˆ Chat Template çš„æ ¼å¼
    messages = [{"role": "system", "content": sys_prompt}]
    
    # å°‡æ­·å²å°è©±åŠ å…¥
    for msg in history:
        messages.append({"role": msg["role"], "content": msg["content"]})
    
    # å¦‚æœæœ‰æª”æ¡ˆå…§å®¹ï¼Œå°‡å…¶çµ„åˆé€² Prompt ä¸­
    full_user_input = prompt
    if file_context:
        full_user_input = f"""I have uploaded a file. Here is the content:
        
=== BEGIN FILE CONTENT ===
{file_context}
=== END FILE CONTENT ===

User Question: {prompt}
"""
    
    # åŠ å…¥ç•¶å‰ä½¿ç”¨è€…è¼¸å…¥
    messages.append({"role": "user", "content": full_user_input})

    inputs = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    
    # æ³¨æ„ï¼šå¦‚æœæª”æ¡ˆå¤ªé•·ï¼Œé€™è£¡å¯èƒ½æœƒè¶…éæ¨¡å‹ä¸Šé™ï¼Œå¯¦éš›ç”Ÿç”¢ç’°å¢ƒéœ€è¦åšæˆªæ–·è™•ç†
    inputs_tokenized = tokenizer(inputs, return_tensors="pt")
    input_ids = inputs_tokenized["input_ids"].to(DEVICE)
    
    do_sample = True
    current_temp = temperature
    if temperature == 0:
        do_sample = False
        current_temp = None 

    generation_args = {
        "max_new_tokens": max_new_tokens,
        "temperature": current_temp,
        "repetition_penalty": repetition_penalty,
        "do_sample": do_sample,
        "use_cache": True,
        "eos_token_id": tokenizer.eos_token_id,
        "pad_token_id": tokenizer.pad_token_id,
    }

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            **generation_args,
        )
    
    response = tokenizer.decode(
        outputs[0][input_ids.shape[1]:], 
        skip_special_tokens=True 
    )
    
    return response

# --- è™•ç†ä½¿ç”¨è€…è¼¸å…¥ ---
if prompt := st.chat_input("è«‹è¼¸å…¥é—œæ–¼è³‡å®‰çš„å•é¡Œ..."):
    
    # 1. è™•ç†æª”æ¡ˆ
    file_text = None
    display_prompt = prompt # åœ¨ç•«é¢ä¸Šé¡¯ç¤ºçš„æ–‡å­—
    
    if uploaded_file:
        with st.spinner("æ­£åœ¨è®€å–æª”æ¡ˆå…§å®¹..."):
            file_text = process_file_content(uploaded_file)
            if file_text:
                # å¦‚æœæœ‰æª”æ¡ˆï¼Œæˆ‘å€‘åœ¨ç•«é¢ä¸ŠåŠ å€‹å°æç¤ºï¼Œä½†ä¸è¦æŠŠæ•´å€‹æª”æ¡ˆå…§å®¹å°å‡ºä¾†æ´—ç‰ˆ
                display_prompt = f"ğŸ“„ **[å·²é™„åŠ æª”æ¡ˆ: {uploaded_file.name}]**\n\n{prompt}"
                # ç°¡å–®çš„é•·åº¦æª¢æŸ¥è­¦å‘Š
                if len(file_text) > 20000:
                    st.toast("âš ï¸ æª”æ¡ˆå…§å®¹è¼ƒé•·ï¼Œå¯èƒ½æœƒè¶…éæ¨¡å‹è™•ç†ä¸Šé™ã€‚", icon="âš ï¸")

    # 2. é¡¯ç¤ºä½¿ç”¨è€…è¨Šæ¯
    st.chat_message("user").markdown(display_prompt)
    
    # 3. å‘¼å«æ¨¡å‹ç”¢ç”Ÿå›æ‡‰
    if model and tokenizer:
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            with st.spinner("æ­£åœ¨åˆ†æèˆ‡æ€è€ƒä¸­..."):
                # å‚³å…¥ file_text ä½œç‚ºé¡å¤–ä¸Šä¸‹æ–‡
                response = generate_response(prompt, st.session_state.messages, system_prompt, file_context=file_text)
                message_placeholder.markdown(response)
        
        # 4. æ›´æ–°å°è©±æ­·å²
        # é€™è£¡æˆ‘å€‘é¸æ“‡å„²å­˜ display_promptï¼Œè®“æ­·å²ç´€éŒ„çœ‹å¾—åˆ°æœ‰å‚³æª”æ¡ˆï¼Œä½†æ¨¡å‹å¯¦éš›ä¸Šæ˜¯æ”¶åˆ°å®Œæ•´æ–‡å­—
        # æ³¨æ„ï¼šç‚ºäº†ç¯€çœ Contextï¼Œæ­·å²ç´€éŒ„è£¡æˆ‘å€‘ä¸å­˜å®Œæ•´çš„æª”æ¡ˆå…§å®¹ï¼Œåªå­˜ä½¿ç”¨è€…çš„å•é¡Œ
        # å¦‚æœå¸Œæœ›æ¨¡å‹åœ¨"ä¸‹ä¸€è¼ª"å°è©±é‚„è¨˜å¾—æª”æ¡ˆï¼Œå‰‡å¿…é ˆå°‡ full content å­˜å…¥ historyï¼Œä½†é€™æœƒæ¶ˆè€—å¤§é‡è¨˜æ†¶é«”
        st.session_state.messages.append({"role": "user", "content": display_prompt})
        st.session_state.messages.append({"role": "assistant", "content": response})
